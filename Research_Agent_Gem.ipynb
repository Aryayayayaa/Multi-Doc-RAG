{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "PooUN9cMASAK",
        "tuO-J82lAQ-8",
        "gCX9965dhzqZ",
        "jIEH1Gi4AcuE"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryayayayaa/Multi-Doc-RAG/blob/main/Research_Agent_Gem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name - Multi-Document Research Agent (RAG + Planning)**    \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is a multi-document research agent built in Python within a Google Colab environment. The system's core function is to intelligently answer complex, natural language questions by orchestrating a hybrid retrieval process that leverages both a local document repository and real-time web search capabilities. It uses an agentic framework to plan and execute multi-step reasoning, ensuring that the final output is a structured, comprehensive, and traceable report."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Aryayayayaa/Multi-Doc-RAG/blob/main/Research_Agent_Gem.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The central challenge this project addresses is the need for a sophisticated AI research assistant that can synthesize information from disparate sources to provide accurate and context-rich answers. Traditional search methods are often limited to either a single document corpus or the public web. This project solves that limitation by building a system that can seamlessly access and combine knowledge from a private, local repository of documents with up-to-the-minute information retrieved from the internet, all while providing full traceability for every claim made."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code -**\n"
      ],
      "metadata": {
        "id": "CGBW95nignaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Necessary pakages\n",
        "!pip install pypdf sentence-transformers faiss-cpu langchain-community openai openai-agents serpapi\n",
        "!pip install google-search-results\n",
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "LE4DCdlFgn-k",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading Dataset for Local Search\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wsdNhtCihKQ-",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set up API Keys as Environment Variables\n",
        "import os\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Specify the path to your.env file in Google Drive\n",
        "dotenv_path = '/content/drive/My Drive/RAG Agent Workspace/secrets.env'\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if os.path.exists(dotenv_path):\n",
        "    load_dotenv(dotenv_path)\n",
        "    print(\"API keys loaded from Google Drive.\")\n",
        "else:\n",
        "    print(\"Warning:.env file not found. Please create one with your API keys.\")\n",
        "    print(\"Path expected: \" + dotenv_path)\n",
        "\n",
        "# You can now access your keys via os.getenv()\n",
        "# For example:\n",
        "# openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# serpapi_key = os.getenv(\"SERPAPI_API_KEY\")"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "A_4qFJahj7Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Load and Chunk Documents\n",
        "\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Define the path to your documents in Google Drive\n",
        "# You should place your 9 documents in a folder, for example: \"local_docs\"\n",
        "# Adjust the path to where you have saved your files.\n",
        "docs_path = '/content/drive/My Drive/RAG Agent Workspace/local_docs'\n",
        "\n",
        "# Define the loader for your documents. We'll use PyPDFLoader for PDF files.\n",
        "# If you have markdown (.md) or text (.txt) files, you would use a different loader like UnstructuredMarkdownLoader\n",
        "# or TextLoader. The DirectoryLoader can handle multiple file types.\n",
        "loader = DirectoryLoader(docs_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "# Load the documents from the specified directory\n",
        "print(\"Loading documents from Google Drive...\")\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "# Initialize the text splitter to create chunks with a specific size and overlap\n",
        "# A chunk_size of 500 characters and an overlap of 50 characters is a good starting point.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "print(\"Splitting documents into chunks...\")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "print(f\"Created {len(texts)} text chunks.\")\n",
        "\n",
        "# The 'texts' variable now contains a list of all the document chunks\n",
        "# and is ready for the next step."
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "POLenqM9kynK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate Embeddings and Build FAISS Index\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "# Initialize the Sentence Transformer model to create embeddings\n",
        "# 'all-MiniLM-L6-v2' is a small, fast, and effective model for this task.\n",
        "print(\"Initializing Sentence Transformer model...\")\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create a FAISS vector store from the document chunks and embeddings\n",
        "# This is where the index is built and the document vectors are stored.\n",
        "print(\"Creating FAISS vector store from chunks...\")\n",
        "vector_store = FAISS.from_documents(texts, embeddings)\n",
        "print(\"Vector store created successfully.\")"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "wRv5Nov3nswq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Save the Index for Future Use\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the path where you want to save your FAISS index\n",
        "# This will save the index and its metadata to your Google Drive.\n",
        "index_path = '/content/drive/My Drive/RAG Agent Workspace/faiss_index'\n",
        "\n",
        "print(f\"Saving FAISS index to {index_path}...\")\n",
        "vector_store.save_local(index_path)\n",
        "print(\"FAISS index saved successfully.\")\n",
        "\n",
        "# To reload the index in a new session, you can use the following code:\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "# reloaded_vector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "# print(\"FAISS index reloaded from Google Drive.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v80N3Zubn0ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the Local Document Search Tool\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.agents import tool\n",
        "import os\n",
        "\n",
        "# Ensure the Sentence Transformer model is initialized again to reload the index\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Define the path where the FAISS index is saved\n",
        "index_path = '/content/drive/My Drive/RAG Agent Workspace/faiss_index'\n",
        "\n",
        "print(\"Attempting to load FAISS index from Google Drive...\")\n",
        "try:\n",
        "    reloaded_vector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "    print(\"FAISS index loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading FAISS index: {e}\")\n",
        "    print(\"Please ensure you have run the previous cells to create and save the index.\")\n",
        "    reloaded_vector_store = None # Set to None to prevent subsequent errors\n",
        "\n",
        "# Define a function to perform the local search with a proper docstring\n",
        "@tool\n",
        "def local_document_search(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the local FAISS vector store for relevant document chunks.\n",
        "    Use this tool to find information from the provided local document repository.\n",
        "    \"\"\"\n",
        "    if reloaded_vector_store:\n",
        "        # Perform a similarity search and get the top 4 most relevant chunks\n",
        "        docs = reloaded_vector_store.similarity_search(query, k=4)\n",
        "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    else:\n",
        "        return \"Local document search is not available. FAISS index not loaded.\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "acAtk6HTqps6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Web Search Tool\n",
        "\n",
        "# Ensure you have installed the necessary libraries:\n",
        "# pip install langchain langchain-community serpapi\n",
        "\n",
        "import os\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "from langchain.agents import tool\n",
        "\n",
        "# Check if the SerpAPI key is set as an environment variable\n",
        "serpapi_api_key = os.environ.get(\"SERPAPI_API_KEY\")\n",
        "if not serpapi_api_key:\n",
        "    raise ValueError(\"SERPAPI_API_KEY environment variable is not set. Please set it before running this cell.\")\n",
        "\n",
        "# Define the SerpAPIWrapper with the API key\n",
        "try:\n",
        "    search = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to initialize SerpAPIWrapper. Check your API key. Error: {e}\")\n",
        "\n",
        "# Define a function to perform the web search\n",
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs a real-time web search for the given query.\n",
        "    Useful for finding up-to-date or general knowledge not available in local documents.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return search.run(query)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during the web search: {e}\"\n",
        "\n",
        "print(\"Web Search Tool 'web_search' created successfully.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5aV6d6900wJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the Agent and Agent Executor\n",
        "\n",
        "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "import os\n",
        "\n",
        "# Check if the OpenAI API key is set\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    print(\"Warning: OPENAI_API_KEY environment variable is not set.\")\n",
        "    print(\"Please run Cell 3 to load your API keys from the.env file.\")\n",
        "\n",
        "# Define the LLM to be used by the agent\n",
        "# 'gpt-4o' is a powerful model that is great at tool use and reasoning.\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Define the tools the agent can use.\n",
        "# Add both the local document search and the web search tools.\n",
        "tools = [local_document_search, web_search] # <-- MODIFIED LINE\n",
        "\n",
        "# Create a prompt for the agent. This instructs the LLM on its role.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\") # This is required for the agent to track thought process\n",
        "])\n",
        "\n",
        "# Create the agent\n",
        "# This binds the LLM and the tools together with the prompt.\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "# Create the AgentExecutor\n",
        "# This is the central runtime that will execute the agent's plan.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "print(\"Agent and AgentExecutor created. You are now ready to run queries.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vCvdNltl0tQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ask the Agent a Question\n",
        "user_question = input(\"Please enter your question for the agent: \")\n",
        "\n",
        "response = agent_executor.invoke({\"input\": user_question})\n",
        "print(response[\"output\"])"
      ],
      "metadata": {
        "id": "N8iLCOxk1uKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applications/Usage**"
      ],
      "metadata": {
        "id": "PooUN9cMASAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hybrid retrieval system can be used in any scenario where a combination of internal, domain-specific knowledge and up-to-date, public information is required. Key applications include:\n",
        "\n",
        "* **Corporate Knowledge Management:** Employees can query internal company documents (e.g., policy manuals, project reports, training materials) and, if the information is insufficient, the system automatically uses a web search to find external best practices or competitive data.\n",
        "* **Customer Support & Helpdesks:** The system can first search a local knowledge base of common issues and solutions. For novel or complex queries, it can then perform a web search to find relevant forums, product documentation, or recent bug fixes.\n",
        "* **Legal & Medical Research:** Researchers can use this to search a private repository of case law or medical journals and seamlessly extend the search to the public web for new legislation or the latest research findings.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xwjBPdDzAZ4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recommendations**"
      ],
      "metadata": {
        "id": "tuO-J82lAQ-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the system's performance and robustness in the short term, consider the following enhancements:\n",
        "\n",
        "* **Implement a more robust Reranking Model:** Replace the current simple reranking with a more advanced model (e.g., a larger CrossEncoder) to more accurately score the relevance of retrieved documents, leading to more precise answers.\n",
        "* **Add Error Handling & Fallbacks:** Implement a robust error-handling mechanism to gracefully manage cases where a tool fails (e.g., web search API is down). The system should be able to fall back to the local documents and inform the user of the issue.\n",
        "* **Improve Local Data Refreshment:** Develop a simple script to periodically check for new documents in the Google Drive folder and update the FAISS index. This ensures the local knowledge base remains current without manual intervention."
      ],
      "metadata": {
        "id": "QhgDii03AbJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully demonstrates the power of a hybrid retrieval-augmented generation (RAG) system. By intelligently combining a local document search with a web search tool, the system can provide comprehensive and well-cited answers that go beyond a single data source. The use of a multi-step agent planner and structured report generation ensures the output is not only accurate but also easy to understand and verify. This architecture serves as a solid foundation for building more complex, domain-specific AI research assistants.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Future Work**"
      ],
      "metadata": {
        "id": "jIEH1Gi4AcuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evolve this project into a more powerful and scalable solution, consider these long-term goals:\n",
        "\n",
        "* **Multi-Agent Collaboration:** Instead of a single agent, implement a multi-agent system where different agents specialize in specific tasks (e.g., a \"Local Search Agent\" and a \"Web Search Agent\"). A \"Planning Agent\" can then orchestrate their collaboration to solve complex, multi-faceted problems.\n",
        "* **Advanced Data Ingestion:** Develop a more sophisticated data pipeline that can ingest and process various data types beyond PDFs, such as web pages, emails, and database records. This would make the system more versatile for diverse use cases.\n",
        "* **User Interface (UI) Development:** Create a web-based user interface to make the system accessible to non-technical users. The UI would provide a simple chat interface for queries and render the final Markdown report in a clean, readable format."
      ],
      "metadata": {
        "id": "Jkttb4wNAic5"
      }
    }
  ]
}